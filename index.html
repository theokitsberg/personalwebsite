<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theo Kitsberg</title>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css?v=2">
</head>
<body>
    <div class="container">
        <header>
            <div class="header-content">
                <div class="text-content">
                    <h1>Theo Kitsberg</h1>
                    <p class="intro">
                        I'm currently studying Philosophy and some Psychology at the
                        <a href="https://www.christs.cam.ac.uk/" target="_blank" rel="noopener noreferrer">University of Cambridge</a>.
                        This summer, I'll be at
                        <a href="https://www.ucl.ac.uk/gatsby/gatsby-computational-neuroscience-unit" target="_blank" rel="noopener noreferrer">UCL's Gatsby Unit</a>
                        for an intensive program in mathematics for machine learning.
                        Following that, I'll be working with
                        <a href="https://nocklab.fas.harvard.edu/people/daniel-low" target="_blank" rel="noopener noreferrer">Daniel Low</a>
                        at the intersection of causal inference, NLP, and crisis response and exploring optimal natural language outputs for people in distress, whilst evaluating how generative AI performs in these contexts.
                    </p>
                    <p class="contact-info">
                        If you want more information about anything here, a chat, or my CV, please
                        <a href="mailto:tck32@cam.ac.uk">email me</a>.
                    </p>
                </div>
                <div class="profile-image">
                    <img src="images/profile.jpg" alt="Profile photo" class="profile-photo">
                </div>
            </div>
        </header>

        <main>
            <section id="interests">
                <h2>Interests</h2>
                <p>
                    As LLMs are increasingly deployed in high-risk and high-impact settings, I'm interested in how we can ensure they behave in a manner which is not only consistent but consistently good, all whilst trying to solve that age-old question of how exactly we ought to understand "good". I am also curious about if we should be willing to give models a pass for less consistent behaviour and, if so, when. For better or for worse, these questions have become time-sensitive.
                </p>
                <p>
                    Currently, I am especially interested in situations which affect mental-wellbeing. Arguably virtually every interaction has the potential to, even if only indirectly, so I spend a decent amount of time thinking about quotidian interactions along with the more crisis-laden ones.

                    More tangentially, I'm also interested in how language models interact with each other and in understanding authenticity (of relationships, work, achievement, identity etc.) in the context of AI developments. 
                </p
            </section>

            <section id="projects">
                <h2>Relevant Work</h2>
                
                <h3>Personal Projects</h3>
                <div class="project">
                    <h4><a href="fourthwallrpo.pdf" target="_blank" rel="noopener noreferrer">Teaching Confidence Calibration Via Recursive Prompt Optimisation (RPO)</a></h4>
                    <p>
                        Built a Recursive Prompt Optimisation (RPO) pipeline that teaches Claude Sonnet 3.7 to calibrate its stated confidence accurately. At each iteration GPT-4o auto-generates hard, multi-turn “red-team” conversations, scores the model purely on calibration quality, and Claude uses that feedback to rewrite its own system-prompt; the loop then repeats, with live graphs tracking training- and hold-out success so we can watch the prompt’s performance climb. Because this method evolves only the prompt (no weight-tuning), it’s fast, fully interpretable, and already shows clear gains from ~8 %.
                    </p>
                </div>

                <div class="project">
                    <h4><a href="moralpsychstability.pdf" target="_blank" rel="noopener noreferrer">Measuring the Stability of LLM Moral Psychology</a></h4>
                    <p>
                        Explored the moral stability of large language models by investigating what I call moral drift. I focused on three levels of evaluation: first, how consistent models are in their moral decision-making when responding to ethical dilemmas under standard versus high-pressure conditions; second, how stable their moral judgments are when those judgments are subjected to direct philosophical critique or pushback; and third, how well a model's own decisions align with its evaluative reasoning—in other words, whether the models judge actions (their own or others') in a way that's coherent with how they make decisions themselves. The goal wasn't to decide if a model is moral, but to understand how its moral reasoning holds together when challenged or strained.
                    </p>
                </div>

                <div class="project">
                    <h4><a href="listeningllama.pdf" target="_blank" rel="noopener noreferrer">Listening Llama</a></h4>
                    <p>
                        Fine-tuned Llama 7B to reliably respond to emotionally charged disclosures in the manner of a helpline volunteer. Generated 1047 synthetic datapoints through review and improvement of batched GPT generations then scaled that dataset to 10,000 via  a haiku API creative rephrasing script. Cleaned the data then fine-tuned llama with instruction-tuning, DPO-tuning and Constitutional AI.
                    </p>
                </div>
                
                <div class="project">
                    <h4><a href="Lawtomate.pdf" target="_blank" rel="noopener noreferrer">Lawtomate: Towards a monolithic, non-iterative prompt for Cambridge Law exams</a></h4>
                    <p>
                        Showing (contrary to popular perception in Cambridge) that students can use LLMs to produce high-quality answers to Cambridge law exams with minimal effort or legal understanding. Moved from API automation to a web UI process and (soon) to a single, monolithic prompt, pushing the technical barrier to entry to the floor. Want to expose equity issues in open-book, online exams.
                    </p>
                </div>

                <div class="project">
                    <h4><a href="Midas Models.pdf" target="_blank" rel="noopener noreferrer">Some Ongoing, Incrementally Updated Thoughts on LLMs in Mental Health</a></h4>
                    <p>
                        My attempt to unravel what we should think and feel about uses of LLMs in behavioural care contexts, like therapy or crisis care.
                    </p>
                </div>

                <h3>Experiences in Industry and Academia</h3>
                <div class="project">
                    <p>
                        Most recently, I have spent time at the <a href="https://cambridge-afar.github.io/" target="_blank" rel="noopener noreferrer">Affective Intelligence and Robotics Lab</a>, testing affect integration in <a href="https://cambridge-afar.github.io/" target="_blank" rel="noopener noreferrer">LEXI</a> to reduce benevolent prejudice and deriving RL reward functions for group HRI from social science literature. Before that, I worked at <a href="https://www.cambridgemindtechnologies.com/" target="_blank" rel="noopener noreferrer">Cambridge Mind Technologies</a>, on red-teaming Cami (their LLM for adolescent support) and testing it against competitors. Last summer, at <a href="https://www.lionheart.vc/" target="_blank" rel="noopener noreferrer">Lionheart Ventures</a>, I spent time assessing AI investment risks through LLM-assisted scenario discovery, alongside more traditional sourcing and diligence work. Before Cambridge, I had my first experience in industry at <a href="https://www.deepchecks.com/" target="_blank" rel="noopener noreferrer">Deepchecks</a>, where I worked on LLM grounding and query avoidance through red-teaming.
                    </p>
                </div>
            </section>

            <section id="hobbies">
                <h2>Hobbies</h2>
                <p>
                    I like to run, and my favourite distance sits somewhere between 10 and 20 km. I'm still undecided on whether I prefer the scenic river route in Cambridge or my equally scenic route back home, which, incidentally, runs alongside a main road. Occasionally, I lift weights, and I once tried Brazilian Jiu Jitsu, where I discovered I'm fantastic at being thrown to the floor and strangled. When I was younger, I used to buy and sell used and collectible books to financially support my reading habits. I still dabble if I see something really special, but I prefer libraries to eBay these days.
                </p>
            </section>
        </main>
    </div>
</body>
</html> 
